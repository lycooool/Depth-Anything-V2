{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3274276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "模型架構總覽\n",
      "================================================================================\n",
      "DepthAnythingV2(\n",
      "  (pretrained): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (depth_heads): ModuleList(\n",
      "    (0-3): 4 x DPTHead(\n",
      "      (projects): ModuleList(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2-3): 2 x Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (resize_layers): ModuleList(\n",
      "        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
      "        (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "        (2): Identity()\n",
      "        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "      (scratch): Module(\n",
      "        (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (refinenet1): FeatureFusionBlock(\n",
      "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resConfUnit1): ResidualConvUnit(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (resConfUnit2): ResidualConvUnit(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (refinenet2): FeatureFusionBlock(\n",
      "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resConfUnit1): ResidualConvUnit(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (resConfUnit2): ResidualConvUnit(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (refinenet3): FeatureFusionBlock(\n",
      "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resConfUnit1): ResidualConvUnit(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (resConfUnit2): ResidualConvUnit(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (refinenet4): FeatureFusionBlock(\n",
      "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resConfUnit1): ResidualConvUnit(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (resConfUnit2): ResidualConvUnit(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (output_conv2): Sequential(\n",
      "          (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): ReLU(inplace=True)\n",
      "          (4): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from depth_anything_v2.dpt_finetune import DepthAnythingV2\n",
    "\n",
    "# 建立模型\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if DEVICE == 'cpu':\n",
    "    print(\"⚠️ 警告：CUDA 不可用，模型將在 CPU 上運行。\")\n",
    "model = DepthAnythingV2(encoder='vitl')\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"模型架構總覽\")\n",
    "print(\"=\" * 80)\n",
    "print(model)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a07232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "4 個獨立的 Depth Heads\n",
      "================================================================================\n",
      "\n",
      "--- Head 0 ---\n",
      "類型: DPTHead\n",
      "參數量: 30,947,009\n",
      "\n",
      "--- Head 1 ---\n",
      "類型: DPTHead\n",
      "參數量: 30,947,009\n",
      "\n",
      "--- Head 2 ---\n",
      "類型: DPTHead\n",
      "參數量: 30,947,009\n",
      "\n",
      "--- Head 3 ---\n",
      "類型: DPTHead\n",
      "參數量: 30,947,009\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 詳細查看 4 個 depth head\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4 個獨立的 Depth Heads\")\n",
    "print(\"=\" * 80)\n",
    "for i, head in enumerate(model.depth_heads):\n",
    "    print(f\"\\n--- Head {i} ---\")\n",
    "    print(f\"類型: {type(head).__name__}\")\n",
    "    num_params = sum(p.numel() for p in head.parameters())\n",
    "    print(f\"參數量: {num_params:,}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce56ac55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "各模組參數統計\n",
      "================================================================================\n",
      "Encoder (DINOv2): 304,368,640 參數\n",
      "Depth Head 0: 30,947,009 參數\n",
      "Depth Head 1: 30,947,009 參數\n",
      "Depth Head 2: 30,947,009 參數\n",
      "Depth Head 3: 30,947,009 參數\n",
      "\n",
      "總參數量: 428,156,676\n",
      "Encoder 比例: 71.1%\n",
      "所有 Heads 比例: 28.9%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 查看 encoder 和各部分的參數量\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"各模組參數統計\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "encoder_params = sum(p.numel() for p in model.pretrained.parameters())\n",
    "print(f\"Encoder (DINOv2): {encoder_params:,} 參數\")\n",
    "\n",
    "total_head_params = 0\n",
    "for i, head in enumerate(model.depth_heads):\n",
    "    head_params = sum(p.numel() for p in head.parameters())\n",
    "    total_head_params += head_params\n",
    "    print(f\"Depth Head {i}: {head_params:,} 參數\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n總參數量: {total_params:,}\")\n",
    "print(f\"Encoder 比例: {encoder_params/total_params*100:.1f}%\")\n",
    "print(f\"所有 Heads 比例: {total_head_params/total_params*100:.1f}%\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25116f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "單一 Depth Head 的詳細架構 (以 Head 0 為例)\n",
      "================================================================================\n",
      "DPTHead(\n",
      "  (projects): ModuleList(\n",
      "    (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2-3): 2 x Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (resize_layers): ModuleList(\n",
      "    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (2): Identity()\n",
      "    (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      "  (scratch): Module(\n",
      "    (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (refinenet1): FeatureFusionBlock(\n",
      "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (resConfUnit1): ResidualConvUnit(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (resConfUnit2): ResidualConvUnit(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (refinenet2): FeatureFusionBlock(\n",
      "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (resConfUnit1): ResidualConvUnit(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (resConfUnit2): ResidualConvUnit(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (refinenet3): FeatureFusionBlock(\n",
      "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (resConfUnit1): ResidualConvUnit(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (resConfUnit2): ResidualConvUnit(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (refinenet4): FeatureFusionBlock(\n",
      "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (resConfUnit1): ResidualConvUnit(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (resConfUnit2): ResidualConvUnit(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (output_conv2): Sequential(\n",
      "      (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 查看單一 depth head 的詳細架構\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"單一 Depth Head 的詳細架構 (以 Head 0 為例)\")\n",
    "print(\"=\" * 80)\n",
    "print(model.depth_heads[0])\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae426b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "測試 Forward Pass\n",
      "================================================================================\n",
      "輸入形狀: torch.Size([1, 3, 518, 518])\n",
      "<class 'list'>\n",
      "\n",
      "輸出類型: <class 'list'>\n",
      "輸出數量: 4 個 depth maps\n",
      "Head 0 輸出形狀: torch.Size([1, 518, 518])\n",
      "Head 1 輸出形狀: torch.Size([1, 518, 518])\n",
      "Head 2 輸出形狀: torch.Size([1, 518, 518])\n",
      "Head 3 輸出形狀: torch.Size([1, 518, 518])\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 測試 forward pass 的輸出形狀\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"測試 Forward Pass\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 建立假的輸入\n",
    "dummy_input = torch.randn(1, 3, 518, 518).cuda()\n",
    "print(f\"輸入形狀: {dummy_input.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(dummy_input)\n",
    "\n",
    "print(type(outputs))\n",
    "\n",
    "print(f\"\\n輸出類型: {type(outputs)}\")\n",
    "print(f\"輸出數量: {len(outputs)} 個 depth maps\")\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"Head {i} 輸出形狀: {output.shape}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Depth-Anything-V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
